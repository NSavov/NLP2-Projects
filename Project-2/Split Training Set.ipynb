{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import operator\n",
    "from __future__ import print_function\n",
    "\n",
    "training_file = \"data/training.zh-en\"\n",
    "lexicon_file = \"lexicon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the estimated IBM1 translation probabilities from the lexicon file\n",
    "\n",
    "with open(lexicon_file) as f:\n",
    "    dictionary_lines = f.read().splitlines()\n",
    "\n",
    "\n",
    "translation_probs_ZH_to_EN = {}\n",
    "translation_probs_EN_to_ZH = {}\n",
    "\n",
    "for line in dictionary_lines:\n",
    "    entries = line.split(' ')\n",
    "    if (entries[0] not in translation_probs_ZH_to_EN):\n",
    "        translation_probs_ZH_to_EN[entries[0]] = {}\n",
    "    if (entries[1] not in translation_probs_EN_to_ZH):\n",
    "        translation_probs_EN_to_ZH[entries[1]] = {}\n",
    "    if (entries[2] != \"NA\"):\n",
    "        translation_probs_ZH_to_EN[entries[0]][entries[1]] = float(entries[2])\n",
    "    if (entries[3] != \"NA\"):\n",
    "        translation_probs_EN_to_ZH[entries[1]][entries[0]] = float(entries[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0052879005670547485"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#translation_probs_ZH_to_EN[\"<NULL>\"]\n",
    "translation_probs_EN_to_ZH[\"<NULL>\"][\"在\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select only the top_n most likely translation for each source (Chinese) word\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "top_n_translation_probs_ZH_to_EN = {}\n",
    "\n",
    "for entry in translation_probs_ZH_to_EN:\n",
    "    new_entry = dict(sorted(translation_probs_ZH_to_EN[entry].iteritems(), key=operator.itemgetter(1), reverse=True)[:top_n])\n",
    "    top_n_translation_probs_ZH_to_EN[entry] = new_entry\n",
    "    \n",
    "    \n",
    "\n",
    "top_n_translation_probs_EN_to_ZH = {}\n",
    "\n",
    "for entry in translation_probs_EN_to_ZH:\n",
    "    new_entry = dict(sorted(translation_probs_EN_to_ZH[entry].iteritems(), key=operator.itemgetter(1), reverse=True)[:top_n])\n",
    "    top_n_translation_probs_EN_to_ZH[entry] = new_entry\n",
    "    \n",
    "    \n",
    "pickle.dump(top_n_translation_probs_ZH_to_EN, open('data/top' + str(top_n) + '_translation_probs_ZH_to_EN.mem', 'wb'))\n",
    "pickle.dump(top_n_translation_probs_EN_to_ZH, open('data/top' + str(top_n) + '_translation_probs_EN_to_ZH.mem', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'s\": 0.04251924529671669,\n",
       " '?': 0.024504011496901512,\n",
       " 'at': 0.07681587338447571,\n",
       " 'be': 0.012425635010004044,\n",
       " 'can': 0.03349512442946434,\n",
       " 'i': 0.07119250297546387,\n",
       " 'in': 0.11082739382982254,\n",
       " 'is': 0.10617364197969437,\n",
       " 'the': 0.4110737442970276,\n",
       " 'where': 0.028134679421782494}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_translation_probs_ZH_to_EN[\"在\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44016\n"
     ]
    }
   ],
   "source": [
    "# Loads the paired sentences from the data file\n",
    "\n",
    "with open(training_file) as f:\n",
    "    paired_sentences = f.read().splitlines()\n",
    "    \n",
    "print(len(paired_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum target sentence length: 65\n",
      "['\"', 'y', '\"', 'as', 'in', 'yokohama', ',', '\"', 'a', '\"', 'as', 'in', 'america', ',', '\"', 'm', '\"', 'as', 'in', 'mexico', ',', '\"', 'a', '\"', 'as', 'in', 'america', ',', '\"', 'g', '\"', 'as', 'in', 'germany', ',', '\"', 'u', '\"', 'as', 'in', 'union', ',', '\"', 'c', '\"', 'as', 'in', 'china', ',', '\"', 'h', '\"', 'as', 'in', 'hong', 'kong', ',', 'and', '\"', 'i', '\"', 'as', 'in', 'italy', '.']\n",
      "\n",
      "Number of excluded sentence pairs given target sentence length constraint: 63\n",
      "Number of remaining sentence pairs given target sentence length constraint: 43953\n"
     ]
    }
   ],
   "source": [
    "# Calculating longest sentence size for target language\n",
    "# Calculating number of target sentences with length greater than a certain value\n",
    "\n",
    "target_sentence_size_value_constraint = 30 #INCLUDING\n",
    "\n",
    "max_ = 0\n",
    "count = 0\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    english_side = pair_sentence[1].split(' ')\n",
    "    if (len(english_side) > max_):\n",
    "        max_ = len(english_side)\n",
    "        max_sentence = english_side\n",
    "    if (len(english_side) > target_sentence_size_value_constraint):\n",
    "        count = count + 1\n",
    "        \n",
    "print(\"Maximum target sentence length: \" + str(max_))\n",
    "print(max_sentence)\n",
    "print(\"\\n\" + \"Number of excluded sentence pairs given target sentence length constraint: \" + str(count))\n",
    "print(\"Number of remaining sentence pairs given target sentence length constraint: \" + \n",
    "      str(len(paired_sentences) - count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining sentence pairs given target sentence length constraint: 43953\n"
     ]
    }
   ],
   "source": [
    "# Selects only those paired sentences whose target sentence length\n",
    "# is smaller than the specified length constraint\n",
    "\n",
    "target_size_constrained_paired_sentences = []\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    english_side = pair_sentence[1].split(' ')\n",
    "    if (len(english_side) <= target_sentence_size_value_constraint):\n",
    "        target_size_constrained_paired_sentences.append(pair)\n",
    "        \n",
    "print(\"Number of remaining sentence pairs given target sentence length constraint: \" +\n",
    "      str(len(target_size_constrained_paired_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to UNK... 100.0% sentences processed so far."
     ]
    }
   ],
   "source": [
    "# Conversion to -UNK- of unobserved types in the constrained top-n lexicon\n",
    "\n",
    "\n",
    "target_size_and_UNK_constrained_paired_sentences = []\n",
    "\n",
    "number_of_sentences = len(target_size_constrained_paired_sentences)\n",
    "\n",
    "for i, pair in enumerate(target_size_constrained_paired_sentences):\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    chinese_side = pair_sentence[0].split(' ')\n",
    "    english_side = pair_sentence[1].split(' ')\n",
    "\n",
    "\n",
    "    english_side_set = set(english_side)\n",
    "\n",
    "    chinese_UNK_sentence = []\n",
    "    english_UNK_sentence = []\n",
    "\n",
    "    set_of_possible_translations = []\n",
    "    for chinese_word in chinese_side:\n",
    "        possible_translations_for_chinese_word = []\n",
    "        for key in top_n_translation_probs_ZH_to_EN[chinese_word]:\n",
    "            possible_translations_for_chinese_word.append(key)\n",
    "            set_of_possible_translations.append(key)\n",
    "        if(len(list(set(possible_translations_for_chinese_word) & english_side_set)) == 0):\n",
    "            chinese_UNK_sentence.append('-UNK-')\n",
    "        else:\n",
    "            chinese_UNK_sentence.append(chinese_word)\n",
    "\n",
    "\n",
    "    set_of_possible_translations = set(set_of_possible_translations)\n",
    "\n",
    "    for english_word in english_side:\n",
    "        if(english_word not in set_of_possible_translations):\n",
    "            english_UNK_sentence.append('-UNK-')\n",
    "        else:\n",
    "            english_UNK_sentence.append(english_word)\n",
    "\n",
    "    new_pair = chinese_UNK_sentence\n",
    "    new_pair.append('|||')\n",
    "    new_pair = new_pair + english_UNK_sentence\n",
    "\n",
    "    new_pair = ' '.join(str(e) for e in new_pair)\n",
    "    target_size_and_UNK_constrained_paired_sentences.append(new_pair)\n",
    "\n",
    "    if (i % 10 == 0 or i + 1 == number_of_sentences):\n",
    "        print('\\r' + 'Converting to UNK... ' + str(100.0*(i+1)/number_of_sentences) + '% sentences processed so far.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determines the partition limits of the training data\n",
    "# for dividing the original data file into 3 similar parts\n",
    "\n",
    "\n",
    "number_of_training_examples = len(target_size_and_UNK_constrained_paired_sentences)\n",
    "\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    number_of_subset_training_examples = number_of_training_examples/3\n",
    "else:\n",
    "    number_of_subset_training_examples12 = number_of_training_examples/3\n",
    "    number_of_subset_training_examples3 = number_of_training_examples - 2*number_of_subset_training_examples12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates 3 new training data files, which are\n",
    "# basically disjoint sets of the original file\n",
    "# and where their union is the entire training data (on constrained length)\n",
    "\n",
    "\n",
    "low_limit = 0\n",
    "\n",
    "# First Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = number_of_subset_training_examples12\n",
    "\n",
    "f = open('data/training_subset1_size' + str(target_sentence_size_value_constraint) + \n",
    "         '_top' + str(top_n) + '.zh-en', 'w')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(target_size_and_UNK_constrained_paired_sentences[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Second Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = 2*number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = 2*number_of_subset_training_examples12\n",
    "\n",
    "f = open('data/training_subset2_size' + str(target_sentence_size_value_constraint) + \n",
    "         '_top' + str(top_n) + '.zh-en', 'w')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(target_size_and_UNK_constrained_paired_sentences[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Third Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = 3*number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = 2*number_of_subset_training_examples12 + number_of_subset_training_examples3\n",
    "\n",
    "f = open('data/training_subset3_size' + str(target_sentence_size_value_constraint) + \n",
    "         '_top' + str(top_n) + '.zh-en', 'w')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(target_size_and_UNK_constrained_paired_sentences[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the original training set into seperate chinese and english corpus\n",
    "\n",
    "chinese_corpus = []\n",
    "english_corpus = []\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    chinese_corpus.append(pair_sentence[0])\n",
    "    english_corpus.append(pair_sentence[1])\n",
    "    \n",
    "    \n",
    "f = open('data/chinese.zh-en', 'w')\n",
    "for entry in chinese_corpus:\n",
    "    f.write(entry)\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open('data/english.zh-en', 'w')\n",
    "for entry in english_corpus:\n",
    "    f.write(entry)\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
