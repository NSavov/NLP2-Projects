{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import operator\n",
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "training_file = \"data/training.zh-en\"\n",
    "lexicon_file = \"lexicon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_n = 5 #INCLUDING\n",
    "top_n_NULL = 2 #INCLUDING\n",
    "sentence_size_value_constraint = 10 #INCLUDING\n",
    "percentage_of_one_occurence_words_to_UNK = 0.5\n",
    "UNK = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads the paired sentences from the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44016\n"
     ]
    }
   ],
   "source": [
    "with open(training_file, encoding='utf8') as f:\n",
    "    paired_sentences = f.read().splitlines()\n",
    "    \n",
    "print(len(paired_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指甲锉\n"
     ]
    }
   ],
   "source": [
    "source_TF = {}\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    chinese_side = pair_sentence[0].split(' ')\n",
    "    for source_word in chinese_side:\n",
    "        if source_word not in source_TF:\n",
    "            source_TF[source_word] = 1\n",
    "        else:\n",
    "            source_TF[source_word] += 1\n",
    "\n",
    "one_word_occurrences = []\n",
    "\n",
    "for key in source_TF:\n",
    "    if source_TF[key] == 1:\n",
    "        one_word_occurrences.append(key)\n",
    "\n",
    "#print(len(list(source_TF.keys())))\n",
    "number_of_one_word_occurrences = len(one_word_occurrences)\n",
    "#print(number_of_one_word_occurrences)\n",
    "number_of_words_mapped_to_UNK = int(number_of_one_word_occurrences*percentage_of_one_occurence_words_to_UNK)\n",
    "#print(number_of_words_mapped_to_UNK)\n",
    "\n",
    "random.shuffle(one_word_occurrences)\n",
    "#print(one_word_occurrences)\n",
    "\n",
    "words_mapped_to_UNK = one_word_occurrences[:number_of_words_mapped_to_UNK]\n",
    "print(words_mapped_to_UNK[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the estimated IBM1 translation probabilities from the lexicon file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(lexicon_file, encoding='utf8') as f:\n",
    "    dictionary_lines = f.read().splitlines()\n",
    "\n",
    "\n",
    "translation_probs_ZH_to_EN = {}\n",
    "translation_probs_EN_to_ZH = {}\n",
    "\n",
    "for line in dictionary_lines:\n",
    "    entries = line.split(' ')\n",
    "    if (entries[0] not in translation_probs_ZH_to_EN and entries[0] not in words_mapped_to_UNK):\n",
    "        translation_probs_ZH_to_EN[entries[0]] = {}\n",
    "    if (entries[1] not in translation_probs_EN_to_ZH):\n",
    "        translation_probs_EN_to_ZH[entries[1]] = {}\n",
    "    if (entries[2] != \"NA\" and entries[0] not in words_mapped_to_UNK):\n",
    "        translation_probs_ZH_to_EN[entries[0]][entries[1]] = float(entries[2])\n",
    "    if (entries[3] != \"NA\"):\n",
    "        translation_probs_EN_to_ZH[entries[1]][entries[0]] = float(entries[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0052879005670547485\n",
      "['wiper', 'au', 'shin', 'produced', 'confirm', 'spouses', 'two-year', 'climbs', 'ache', 'defective', 'norm', 'pants', 'oranges', 'shaded', 'gallons', 'souvenirs', 'slim', 'presently', 'magnifying', 'finest']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'指甲锉'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-bc9cfe4c2e11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_probs_EN_to_ZH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<NULL>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"在\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_probs_ZH_to_EN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<NULL>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NOT ORDERED BY MOST LIKELY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation_probs_ZH_to_EN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'指甲锉'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NOT ORDERED BY MOST LIKELY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '指甲锉'"
     ]
    }
   ],
   "source": [
    "# TEST CELL\n",
    "\n",
    "#translation_probs_ZH_to_EN[\"<NULL>\"]\n",
    "print(translation_probs_EN_to_ZH[\"<NULL>\"][\"在\"])\n",
    "print(list(translation_probs_ZH_to_EN[\"<NULL>\"])[:20]) # NOT ORDERED BY MOST LIKELY\n",
    "print(list(translation_probs_ZH_to_EN['指甲锉'])[:20]) # NOT ORDERED BY MOST LIKELY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select only the top_n most likely translations for each source word (and the top_n_NULL most likely translations for the NULL symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_n_translation_probs_ZH_to_EN = {}\n",
    "\n",
    "for entry in translation_probs_ZH_to_EN:\n",
    "    if entry != '<NULL>':\n",
    "        entry_key = entry\n",
    "        new_entry = dict(sorted(translation_probs_ZH_to_EN[entry].items(), key=operator.itemgetter(1), reverse=True)[:top_n])\n",
    "    else:\n",
    "        #entry_key = '-EPS-'\n",
    "        entry_key = '<NULL>'\n",
    "        new_entry = dict(sorted(translation_probs_ZH_to_EN[entry].items(), key=operator.itemgetter(1), reverse=True)[:top_n_NULL])\n",
    "    \n",
    "    top_n_translation_probs_ZH_to_EN[entry_key] = new_entry\n",
    "    \n",
    "    \n",
    "\n",
    "top_n_translation_probs_EN_to_ZH = {}\n",
    "\n",
    "for entry in translation_probs_EN_to_ZH:\n",
    "    if entry != '<NULL>':\n",
    "        entry_key = entry\n",
    "        new_entry = dict(sorted(translation_probs_EN_to_ZH[entry].items(), key=operator.itemgetter(1), reverse=True)[:top_n])\n",
    "    else:\n",
    "        #entry_key = '-EPS-'\n",
    "        entry_key = '<NULL>'\n",
    "        new_entry = dict(sorted(translation_probs_EN_to_ZH[entry].items(), key=operator.itemgetter(1), reverse=True)[:top_n_NULL])\n",
    "    \n",
    "    top_n_translation_probs_EN_to_ZH[entry_key] = new_entry\n",
    "    \n",
    "    \n",
    "pickle.dump(top_n_translation_probs_ZH_to_EN, open('data/top' + str(top_n) + '_topNULL' + str(top_n_NULL) +\n",
    "                                                   '_%unseen' + str(percentage_of_one_occurence_words_to_UNK) +\n",
    "                                                   '_translation_probs_ZH_to_EN.mem', 'wb'))\n",
    "pickle.dump(top_n_translation_probs_EN_to_ZH, open('data/top' + str(top_n) + '_topNULL' + str(top_n_NULL) + '_translation_probs_EN_to_ZH.mem', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selects only those paired sentences whose target sentence length is smaller than the specified length constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining sentence pairs given target sentence length constraint: 33255\n"
     ]
    }
   ],
   "source": [
    "size_constrained_paired_sentences = []\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    chinese_side = pair_sentence[0].split(' ')\n",
    "    english_side = pair_sentence[1].split(' ')\n",
    "    if (len(english_side) <= sentence_size_value_constraint and len(chinese_side) <= sentence_size_value_constraint):\n",
    "        size_constrained_paired_sentences.append(pair)\n",
    "        \n",
    "print(\"Number of remaining sentence pairs given target sentence length constraint: \" +\n",
    "      str(len(size_constrained_paired_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to -UNK- of unobserved types in the constrained top-n lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to UNK... 100.0% sentences processed so far."
     ]
    }
   ],
   "source": [
    "# THIS NEEDS TO BE CHECKED AND IMPROVED\n",
    "if UNK:\n",
    "    size_and_UNK_constrained_paired_sentences = []\n",
    "\n",
    "    number_of_sentences = len(size_constrained_paired_sentences)\n",
    "\n",
    "    for i, pair in enumerate(size_constrained_paired_sentences):\n",
    "        pair_sentence = pair.split(' ||| ')\n",
    "        chinese_side = pair_sentence[0].split(' ')\n",
    "        english_side = pair_sentence[1].split(' ')\n",
    "\n",
    "\n",
    "        english_side_set = set(english_side)\n",
    "\n",
    "        chinese_UNK_sentence = []\n",
    "        english_UNK_sentence = []\n",
    "\n",
    "        \n",
    "        set_of_possible_translations = []\n",
    "        for key in top_n_translation_probs_ZH_to_EN['<NULL>']:\n",
    "            set_of_possible_translations.append(key)\n",
    "            \n",
    "        for chinese_word in chinese_side:\n",
    "            if chinese_word not in words_mapped_to_UNK:\n",
    "                for key in top_n_translation_probs_ZH_to_EN[chinese_word]:\n",
    "                    set_of_possible_translations.append(key)\n",
    "                chinese_UNK_sentence.append(chinese_word)\n",
    "            else:\n",
    "                chinese_UNK_sentence.append('-UNK-')\n",
    "\n",
    "\n",
    "        set_of_possible_translations = set(set_of_possible_translations)\n",
    "\n",
    "        for english_word in english_side:\n",
    "            if(english_word not in set_of_possible_translations):\n",
    "                english_UNK_sentence.append('-UNK-')\n",
    "            else:\n",
    "                english_UNK_sentence.append(english_word)\n",
    "\n",
    "        new_pair = chinese_UNK_sentence\n",
    "        new_pair.append('|||')\n",
    "        new_pair = new_pair + english_UNK_sentence\n",
    "\n",
    "        new_pair = ' '.join(str(e) for e in new_pair)\n",
    "        size_and_UNK_constrained_paired_sentences.append(new_pair)\n",
    "\n",
    "        if (i % 10 == 0 or i + 1 == number_of_sentences):\n",
    "            print('\\r' + 'Converting to UNK... ' + str(100.0*(i+1)/number_of_sentences) + '% sentences processed so far.', end='')\n",
    "            \n",
    "else:\n",
    "    print('UNK option was not selected! No conversion to UNK performed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determines the partition limits of the training data for dividing the original data file into 3 similar parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if UNK:\n",
    "    paired_sentences_to_use = size_and_UNK_constrained_paired_sentences\n",
    "else:\n",
    "    paired_sentences_to_use = size_constrained_paired_sentences\n",
    "    \n",
    "number_of_training_examples = len(paired_sentences_to_use)\n",
    "\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    number_of_subset_training_examples = number_of_training_examples/3\n",
    "else:\n",
    "    number_of_subset_training_examples12 = number_of_training_examples/3\n",
    "    number_of_subset_training_examples3 = number_of_training_examples - 2*number_of_subset_training_examples12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates 3 new training data files, which are basically disjoint sets of the original file and where their union is the entire training data (on constrained length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if UNK:\n",
    "    unk = '_UNK'\n",
    "else:\n",
    "    unk = '_noUNK'\n",
    "\n",
    "low_limit = 0\n",
    "\n",
    "# First Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = number_of_subset_training_examples12\n",
    "\n",
    "f = open('data/training_subset1_size' + str(sentence_size_value_constraint)+\n",
    "         '_top'+str(top_n)+ '_topNULL'+str(top_n_NULL)+'_%unseen'+\n",
    "         str(percentage_of_one_occurence_words_to_UNK)+unk+'.zh-en', 'w', encoding='utf8')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(paired_sentences_to_use[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Second Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = 2*number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = 2*number_of_subset_training_examples12\n",
    "\n",
    "f = open('data/training_subset2_size' + str(sentence_size_value_constraint)+\n",
    "         '_top'+str(top_n)+ '_topNULL'+str(top_n_NULL)+'_%unseen'+\n",
    "         str(percentage_of_one_occurence_words_to_UNK)+unk+'.zh-en', 'w', encoding='utf8')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(paired_sentences_to_use[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()\n",
    "\n",
    "\n",
    "# Third Subset\n",
    "if(number_of_training_examples % 3 == 0):\n",
    "    high_limit = 3*number_of_subset_training_examples\n",
    "else:\n",
    "    high_limit = 2*number_of_subset_training_examples12 + number_of_subset_training_examples3\n",
    "\n",
    "f = open('data/training_subset3_size' + str(sentence_size_value_constraint)+\n",
    "         '_top'+str(top_n)+ '_topNULL'+str(top_n_NULL)+'_%unseen'+\n",
    "         str(percentage_of_one_occurence_words_to_UNK)+unk+'.zh-en', 'w', encoding='utf8')\n",
    "while(low_limit < high_limit):\n",
    "    f.write(paired_sentences_to_use[low_limit])\n",
    "    f.write('\\n')\n",
    "    low_limit = low_limit + 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the original training set into seperate chinese and english corpus (used for word2vec, maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chinese_corpus = []\n",
    "english_corpus = []\n",
    "\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    chinese_corpus.append(pair_sentence[0])\n",
    "    english_corpus.append(pair_sentence[1])\n",
    "    \n",
    "    \n",
    "f = open('data/chinese.zh-en', 'w')\n",
    "for entry in chinese_corpus:\n",
    "    f.write(entry)\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open('data/english.zh-en', 'w')\n",
    "for entry in english_corpus:\n",
    "    f.write(entry)\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## EXTRA CELLS FOR MOSTLY USELESS STUFF (just for checking, currently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum target sentence length: 65\n",
      "['\"', 'y', '\"', 'as', 'in', 'yokohama', ',', '\"', 'a', '\"', 'as', 'in', 'america', ',', '\"', 'm', '\"', 'as', 'in', 'mexico', ',', '\"', 'a', '\"', 'as', 'in', 'america', ',', '\"', 'g', '\"', 'as', 'in', 'germany', ',', '\"', 'u', '\"', 'as', 'in', 'union', ',', '\"', 'c', '\"', 'as', 'in', 'china', ',', '\"', 'h', '\"', 'as', 'in', 'hong', 'kong', ',', 'and', '\"', 'i', '\"', 'as', 'in', 'italy', '.']\n",
      "\n",
      "Number of excluded sentence pairs given target sentence length constraint: 10761\n",
      "Number of remaining sentence pairs given target sentence length constraint: 33255\n"
     ]
    }
   ],
   "source": [
    "# Calculating longest sentence size for target language\n",
    "# Calculating number of target sentences with length greater than a certain value\n",
    "\n",
    "\n",
    "max_ = 0\n",
    "count = 0\n",
    "for pair in paired_sentences:\n",
    "    pair_sentence = pair.split(' ||| ')\n",
    "    \n",
    "    english_side = pair_sentence[1].split(' ')\n",
    "    chinese_side = pair_sentence[0].split(' ')\n",
    "    if (len(english_side) > max_):\n",
    "        max_ = len(english_side)\n",
    "        max_sentence = english_side\n",
    "    if len(english_side) > sentence_size_value_constraint or (len(chinese_side) > sentence_size_value_constraint):\n",
    "        count = count + 1\n",
    "        \n",
    "print(\"Maximum target sentence length: \" + str(max_))\n",
    "print(max_sentence)\n",
    "print(\"\\n\" + \"Number of excluded sentence pairs given target sentence length constraint: \" + str(count))\n",
    "print(\"Number of remaining sentence pairs given target sentence length constraint: \" + \n",
    "      str(len(paired_sentences) - count))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
